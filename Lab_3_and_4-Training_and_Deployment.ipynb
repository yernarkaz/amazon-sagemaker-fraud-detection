{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Lab 3 and Lab 4: Training and Deployment\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook demonstrate how to train a model and deploy it. We will go through the steps to train your model using the available data and then we'll validate the predictions using a subset of the data. Once we're done validating the data, we'll deploy the model and generate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index\n",
    "\n",
    "---\n",
    "\n",
    "1. [Prerequisites](<#Prerequisites>)\n",
    "1. [Data Handling](#Data-Handling)\n",
    "1. [Lab 3: Train a model using XGBoost](<#Lab-3:-Train-a-Model-using-XGBoost>)\n",
    "    1. [Training: Create and fit the estimator](#Training:-Create-and-fit-the-estimator)\n",
    "    1. [Deposit trained model in SageMaker Model Registry](<#Deposit-trained-model-in-SageMaker-Model-Registry>)\n",
    "1. [Deploy and serve the model](<#Lab-4:-Deploy-and-serve-the-model>)\n",
    "    1. [Evaluate trained model and update status in the model registry](<#Evaluate-trained-model-and-update-status-in-the-model-registry>)\n",
    "    1. [Model deployment](#Model-deployment)\n",
    "    1. [Create/update endpoint](#Create/Update-endpoint)\n",
    "    1. [Predictor interface](#Predictor-interface)\n",
    "1. [(Optional) Clean-up](<#Optional-clean-up>)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "---\n",
    "\n",
    "Install and update required libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Python libraries we are going to use in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1\n",
    "import json\n",
    "import time\n",
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "\n",
    "from sagemaker.xgboost.estimator import XGBoost\n",
    "from scripts.inference_specification import InferenceSpecification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Set general parameters, as region and initiate boto3 and SageMaker SDK variables. You can adjust the code to use a region of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2\n",
    "# Set Region\n",
    "region = sagemaker.Session().boto_region_name\n",
    "print(\"Using AWS Region: {}\".format(region))\n",
    "\n",
    "boto3.setup_default_session(region_name=region)\n",
    "boto_session = boto3.Session(region_name=region)\n",
    "s3_client = boto3.client(\"s3\", region_name=region)\n",
    "\n",
    "# Get SageMaker client, role and session\n",
    "sagemaker_boto_client = boto_session.client(\"sagemaker\")\n",
    "sagemaker_role = sagemaker.get_execution_role()\n",
    "sagemaker_session = sagemaker.session.Session(\n",
    "    boto_session=boto_session, sagemaker_client=sagemaker_boto_client\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set parameterized variables:\n",
    "* bucket - S3 Bucket name. You can adjust the code to use a bucket of your choice.\n",
    "* Prefix - String which will be used to identify different resources.\n",
    "* Training parameters:\n",
    "    * `estimator_output_path` - S3 location for saving the training result (model artifacts and output files).\n",
    "    * `train_instance_count` - Number of Amazon EC2 instances to use for training.\n",
    "    * `train_instance_type` - Type of EC2 instance to use for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3\n",
    "# Bucket \n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = \"fraud-detect-demo\"\n",
    "\n",
    "# Training parameters\n",
    "estimator_output_path = f\"s3://{bucket}/{prefix}/training_jobs\"\n",
    "train_instance_count = 1\n",
    "train_instance_type = \"ml.m5.xlarge\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Handling\n",
    "\n",
    "---\n",
    "\n",
    "In this lab, you will train your machine learning model with the training dataset. There are few ways to obtain the dataset:\n",
    "1. Use the dataset you uploaded to Amazon S3 bucket in the previous Lab (Lab 2 - Store Features in Feature Store). \n",
    "2. Upload the following datasets from `data` folder to Amazon S3: `train.csv`, `test.csv`\n",
    "\n",
    "The following code uploads the datasets from `data` folder to Amazon S3.\n",
    "\n",
    "The code can be adjusted to use datasets created in previous labs or elsewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4\n",
    "# Set data URI for training input\n",
    "train_data_uri = f\"s3://{bucket}/{prefix}/data/train/train.csv\"\n",
    "test_data_uri = f\"s3://{bucket}/{prefix}/data/test/test.csv\"\n",
    "\n",
    "# Upload data files\n",
    "s3_client.upload_file(Filename=\"data/train.csv\", Bucket=bucket, Key=f\"{prefix}/data/train/train.csv\")\n",
    "s3_client.upload_file(Filename=\"data/test.csv\", Bucket=bucket, Key=f\"{prefix}/data/test/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 3: Train a Model using XGBoost\n",
    "\n",
    "---\n",
    "\n",
    "To train a model in SageMaker, you create a training job. The training job includes the following information:\n",
    "1. The URL of the Amazon Simple Storage Service (Amazon S3) bucket where you've stored the training data.\n",
    "2. The compute resources that you want SageMaker to use for model training. Compute resources are ML compute instances that are managed by SageMaker.\n",
    "3. The URL of the S3 bucket where you want to store the output of the job.\n",
    "4. The Amazon Elastic Container Registry path where the training code is stored.\n",
    "\n",
    "Lets walkthrough how the training process works:\n",
    "1. Invoke model.fit() to start the training process  \n",
    "2. Process downloads your training algorithm from ECR\n",
    "3. Training data from S3 is used to train the model and create the model file\n",
    "4. Model file is written to S3 which will be used as input for inferencing in the Deploy process\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![training-job.png](images/notebooks/training-job.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the training and test datasets have been persisted in S3, you can start training a model by defining which SageMaker Estimator you'd like to use. For this guide, you will use the [XGBoost Open Source Framework](https://sagemaker.readthedocs.io/en/stable/frameworks/xgboost/xgboost.html) to train your model. This estimator is accessed via the SageMaker SDK, but mirrors the open source version of the [XGBoost Python package](https://xgboost.readthedocs.io/en/latest/python/index.html). Any functionality provided by the XGBoost Python package can be implemented in your training script.\n",
    "This estimator includes reference to the relevant training algorithm from ECR.\n",
    "\n",
    "\n",
    "XGBoost is an extremely popular, open-source package for gradient boosted trees. It is computationally powerful, fully featured, and has been successfully used in many machine learning competitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set HyperParameters\n",
    "These are the parameters which will be sent to our training script in order to train the model. Although they are all defined as \"hyperparameters\" here, they can encompass XGBoost's [Learning Task Parameters](https://xgboost.readthedocs.io/en/latest/parameter.html#learning-task-parameters), [Tree Booster Parameters](https://xgboost.readthedocs.io/en/latest/parameter.html#parameters-for-tree-booster), or any other parameters you'd like to configure for XGBoost.\n",
    "\n",
    "For this example, we will use the following hyperparameters for the XGBoost algorithm:\n",
    "\n",
    "* `max_depth` - Controls how deep each tree within the algorithm can be built. Deeper trees can lead to better fit, but are more computationally expensive and can lead to overfitting. There is typically some trade-off in model performance that needs to be explored between a large number of shallow trees and a smaller number of deeper trees.\n",
    "* `num_round` - Controls the number of boosting rounds. This is essentially the subsequent models that are trained using the residuals of previous iterations. More rounds should produce a better fit on the training data, but can be computationally expensive or lead to overfitting.\n",
    "* `eta` - Controls how aggressive each round of boosting is. Larger values lead to more conservative boosting.\n",
    "* `objective` - Specifies the learning task and the corresponding learning objective. Use `binary:logistic` for binary classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5\n",
    "hyperparameters = {\n",
    "    \"max_depth\": \"3\",\n",
    "    \"eta\": \"0.2\",\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"num_round\": \"100\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Training: Create and fit the estimator\n",
    "If you want to explore the breadth of functionality offered by the SageMaker XGBoost Framework you can read about all the configuration parameters by referencing the inheriting classes. The XGBoost class inherits from the Framework class and Framework inherits from the EstimatorBase class:\n",
    "* [XGBoost Estimator documentation](https://sagemaker.readthedocs.io/en/stable/frameworks/xgboost/xgboost.html#sagemaker.xgboost.estimator.XGBoost)\n",
    "* [Framework documentation](https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html#sagemaker.estimator.Framework)\n",
    "* [EstimatorBase documentation](https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html#sagemaker.estimator.EstimatorBase)\n",
    "\n",
    "For this example, we will use the following parameters for the XGBoost estimator:\n",
    "* `entry_point` - Path to the Python source file which should be executed as the entry point to training.\n",
    "* `hyperparameters` - Hyperparameters that will be used for training. The hyperparameters are made accessible as a dict[str, str] to the training code on SageMaker.\n",
    "* `output_path` - S3 location for saving the training result (model artifacts and output files).\n",
    "* `framework_version` - XGBoost version you want to use for executing your model training code.\n",
    "* `instance_type` - Type of EC2 instance to use for training.\n",
    "\n",
    "The following code will launch the training job and store the trained model into S3, **the training process should take ~4 minutes**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 6\n",
    "# Creating the SageMaker Estimator object\n",
    "xgb_estimator = XGBoost(\n",
    "    entry_point=\"scripts/xgboost_starter_script.py\",\n",
    "    output_path=estimator_output_path,\n",
    "    code_location=estimator_output_path,\n",
    "    hyperparameters=hyperparameters,\n",
    "    role=sagemaker_role,\n",
    "    instance_count=train_instance_count,\n",
    "    instance_type=train_instance_type,\n",
    "    framework_version=\"1.0-1\",\n",
    ")\n",
    "\n",
    "# Training the model\n",
    "xgb_estimator.fit(inputs={\"train\": train_data_uri})\n",
    "\n",
    "# Get Training job name\n",
    "training_job_name = xgb_estimator.latest_training_job.job_name\n",
    "print(\"Training job name: {}\".format(training_job_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the output includes the value of `Billable seconds`, which is the amount of time you will be actually charge for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deposit trained model in SageMaker Model Registry\n",
    "\n",
    "Once a useful model has been trained and its artifacts properly associated, the next step is to save the model in a registry for future reference and possible deployment. In this section, we will see how you can package your artifacts into a ModelPackage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Model Package Group\n",
    "A Model Package Group holds multiple versions or iterations of a model. Though it is not required to create them for every model in the registry, they help organize various models which all have the same purpose and provide automatic versioning. A Model Package is a reusable model artifact abstraction that packages all ingredients necessary for inference. \n",
    "\n",
    "Setup the Model Package Group name according to the `prefix` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7\n",
    "mpg_name = prefix\n",
    "print(f\"Model Package Group name: {mpg_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Model Package dictionary that will contain all the information on the model, the following cells will add information to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8\n",
    "mpg_input_dict = {\n",
    "    \"ModelPackageGroupName\": mpg_name,\n",
    "    \"ModelPackageGroupDescription\": \"Insurance claim fraud detection\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we already have a Model Package Group with this name, we can use it. If we don't have one, we need to create a new one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9\n",
    "matching_mpg = sagemaker_boto_client.list_model_package_groups(NameContains=mpg_name)['ModelPackageGroupSummaryList']\n",
    "\n",
    "# Check if Model Package Group already exist\n",
    "if matching_mpg:\n",
    "    print(f'Using existing Model Package Group: {mpg_name}')\n",
    "else:\n",
    "    mpg_response = sagemaker_boto_client.create_model_package_group(**mpg_input_dict)\n",
    "    print(f'Create Model Package Group {mpg_name}: SUCCESSFUL')\n",
    "    %store mpg_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a training job description so we can extract relevant information from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10\n",
    "training_job_info = sagemaker_boto_client.describe_training_job(\n",
    "    TrainingJobName=training_job_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model package consists of an inference specification that defines the inference image to use along with an optional model weights location. `InferenceSpecification` module is implemented in `model_package_src` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11\n",
    "mp_inference_spec = InferenceSpecification().get_inference_specification_dict(\n",
    "    ecr_image=training_job_info[\"AlgorithmSpecification\"][\"TrainingImage\"],\n",
    "    supports_gpu=False,\n",
    "    supported_content_types=[\"text/csv\"],\n",
    "    supported_mime_types=[\"text/csv\"],\n",
    ")\n",
    "\n",
    "mp_inference_spec[\"InferenceSpecification\"][\"Containers\"][0][\"ModelDataUrl\"] = training_job_info[\"ModelArtifacts\"][\"S3ModelArtifacts\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding information to the Model Package dictionary, including inference specification, and creating the model package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12\n",
    "mp_input_dict = {\n",
    "    \"ModelPackageGroupName\": mpg_name,\n",
    "    \"ModelPackageDescription\": \"XGBoost classifier to detect insurance fraud.\",\n",
    "    \"ModelApprovalStatus\": \"PendingManualApproval\",\n",
    "}\n",
    "\n",
    "mp_input_dict.update(mp_inference_spec)\n",
    "\n",
    "# Create the Model Package\n",
    "mp_response = sagemaker_boto_client.create_model_package(**mp_input_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop and wait until the model package will be added to the registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13\n",
    "mp_info = sagemaker_boto_client.describe_model_package(\n",
    "    ModelPackageName=mp_response[\"ModelPackageArn\"]\n",
    ")\n",
    "mp_status = mp_info[\"ModelPackageStatus\"]\n",
    "\n",
    "while mp_status not in [\"Completed\", \"Failed\"]:\n",
    "    time.sleep(60)\n",
    "    mp_info = sagemaker_boto_client.describe_model_package(\n",
    "        ModelPackageName = mp_response[\"ModelPackageArn\"]\n",
    "    )\n",
    "    mp_status = mp_info[\"ModelPackageStatus\"]\n",
    "    print(f\"model package status: {mp_status}\")\n",
    "print(f\"model package status: {mp_status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point you will be able to view model package in Model registry. Navigate to the model registry by clicking on the `Home` icon in the left sidebar and then to the `Model` menu.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congratulations! \n",
    "\n",
    "You have finished Lab 3. Please return to workshop studio to understand what's next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "## Lab 4: Deploy and serve the model\n",
    "\n",
    "---\n",
    "\n",
    "Once training is completed, we can deploy the trained model as a real time endpoint using Amazon SageMaker hosting services. This will allow us to make predictions (or inference) from the model. The steps below will demonstrate how to: \n",
    " - Evaluate trained model.\n",
    " - Based on the evaluation results, update the model approval status in the model registry.\n",
    " - Deploy the model as real time endpoint using SageMaker hosting services.\n",
    " - Using Dataset and Claims customer dataset prepared as part of Lab 1, run the inference against sample policy id.\n",
    " \n",
    " This lab will take approximately 10 mins to run.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate trained model and update status in the model registry\n",
    "\n",
    "When we registered the model in the model registry, the default status is \"Pending Approval\" status. In the real-life MLOps lifecycle, a model package gets approved after evaluation by data scientists, subject matter experts and auditors. For the purpose of this lab, we will evaluate the model with test dataset that was created during training process. \n",
    "\n",
    "#### Model Evaluation\n",
    "\n",
    "The following script evaluates the trained model performance metric against a certain threshold value. We will use AUC metric as the model performance metric criteria and set the minimum threshold value as 0.7.\n",
    "\n",
    "Create SageMaker Processing Job providing:\n",
    "- The evaluation script\n",
    "- The model artifact located in S3 (from previous Training job)\n",
    "- The test data located in S3\n",
    "\n",
    "The processing job will approximately take 3 mins to run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 15\n",
    "from sagemaker.processing import (\n",
    "    ProcessingInput,\n",
    "    ProcessingOutput,\n",
    "    ScriptProcessor,\n",
    ")\n",
    "\n",
    "script_processor = ScriptProcessor(command=['python3'],\n",
    "                    base_job_name=prefix,\n",
    "                    image_uri=training_job_info[\"AlgorithmSpecification\"][\"TrainingImage\"],\n",
    "                    role=sagemaker_role,\n",
    "                    instance_count=1,\n",
    "                    instance_type='ml.m5.xlarge')\n",
    "\n",
    "# model.tar.gz S3 location \n",
    "model_s3_uri = training_job_info['ModelArtifacts']['S3ModelArtifacts']\n",
    "report_output_dir = f\"s3://{bucket}/{prefix}/evaluation_jobs\"\n",
    "\n",
    "script_processor.run(\n",
    "    code=\"./scripts/evaluate.py\",\n",
    "    inputs=[\n",
    "        ProcessingInput(source=model_s3_uri, destination=\"/opt/ml/processing/model\"),\n",
    "        ProcessingInput(source=test_data_uri, destination=\"/opt/ml/processing/test\"),\n",
    "    ],\n",
    "    outputs=[ProcessingOutput(output_name=\"evaluation\", source=\"/opt/ml/processing/evaluation\", destination=report_output_dir)],\n",
    ")\n",
    "\n",
    "evaluation_job_description = script_processor.jobs[-1].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 16\n",
    "# Download evaluation score from the Processing job output\n",
    "eval_report = \"./outputs/evaluation.json\"\n",
    "eval_report_dir = evaluation_job_description['ProcessingOutputConfig']['Outputs'][0]['S3Output']['S3Uri'].replace(f\"s3://{bucket}/\", '')\n",
    "s3_client.download_file(bucket, f\"{eval_report_dir}/evaluation.json\", eval_report)\n",
    "\n",
    "with open(eval_report) as f:\n",
    "    model_score_report = json.load(f)\n",
    "\n",
    "print(model_score_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate model AUC score. \n",
    "Based on the results, update model approval status for that version. Please note, we are using API's to Approve/Reject model approval status. You can also update the approval status with the SageMaker Studio UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 17\n",
    "eval_auc_threshold = 0.7\n",
    "model_approval_stat = \"PendingManualApproval\"\n",
    "model_auc_eval_score = model_score_report['binary_classification_metrics']['auc']['value']\n",
    "\n",
    "if model_auc_eval_score >= eval_auc_threshold:\n",
    "    model_approval_stat = \"Approved\"\n",
    "    print(\"Model approved!\")\n",
    "else:\n",
    "    model_approval_stat = \"Rejected\"\n",
    "    print(\"Model rejected!\")\n",
    "    print(\"You should re-evaliuate data or traning script to improve the model AUC\")\n",
    "\n",
    "modelPackageList=sagemaker_boto_client.list_model_packages(ModelPackageGroupName=mpg_name,SortBy=\"CreationTime\", SortOrder=\"Descending\")[\"ModelPackageSummaryList\"]\n",
    "\n",
    "#Get the model packge ARN from  Model Package. \n",
    "model_package_arn = modelPackageList[0][\"ModelPackageArn\"]\n",
    "print(model_package_arn)\n",
    "\n",
    "model_package_update_input_dict = {\n",
    "    \"ModelPackageArn\" : model_package_arn,\n",
    "    \"ModelApprovalStatus\" : model_approval_stat\n",
    "}\n",
    "#update the model package registry with appropriate status\n",
    "model_package_update_response = sagemaker_boto_client.update_model_package(**model_package_update_input_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model deployment\n",
    "In the previous step, the model status was adjudicated based on AUC of the test dataset. If the AUC was above the defined threshold , then the approval status for that particular version is set to \"Approved\". At this stage, the model is ready for deployment. Please note, we do not have explicit step to stop the model deployment if the AUC was below threshold. In the real-life MLOPS lifecycle, the model approval status is set to \"Rejected\" if the model performance is not per the business KPI's and you will either tweak the data processing or training script to improve the model performance. \n",
    "\n",
    "Once the model is approved, it is ready for deployment. We will deploy the model for real time inference using [SageMaker Hosting service](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints.html). In order to deploy the endpoint, we will \n",
    "- Define interface endpoint requirements.\n",
    "- Create Model\n",
    "- Create Endpoint config\n",
    "- Create/Update endpoint using newly created EndpointConfig.\n",
    "- Test the endpoint with sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 18\n",
    "# define end point hosting configuration\n",
    "endpoint_name = f\"{prefix}-endpoint\"\n",
    "endpoint_instance_count = 1\n",
    "endpoint_instance_type = \"ml.m4.xlarge\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Model \n",
    "The code below creates a model object from the model version by calling the create_model method. Pass the Amazon Resource Name (ARN) of the model version as part of the Containers for the model object. You can also deploy the model directly using Model package group, but the aim of deploy example is to demonstrate how you can create model from the model version and create/update existing endpoint.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 19\n",
    "from sagemaker import ModelPackage\n",
    "from time import gmtime, strftime\n",
    "\n",
    "model = ModelPackage(role=sagemaker_role,\n",
    "         model_package_arn=model_package_arn,\n",
    "         sagemaker_session=sagemaker_session)\n",
    "print(model)\n",
    "\n",
    "model_name = prefix+\"-\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(\"Model name : {}\".format(model_name))\n",
    "container_list = [{'ModelPackageName': model_package_arn}]\n",
    "\n",
    "\n",
    "create_model_response = sagemaker_boto_client.create_model( ModelName=model_name,\n",
    "                                         ExecutionRoleArn=sagemaker_role,\n",
    "                                            Containers = container_list\n",
    "                                       )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Endpoint config\n",
    "To host your model, you create an endpoint configuration with the CreateEndpointConfig API, and then create an endpoint with the CreateEndpoint API. SageMaker then deploys all of the containers that you defined for the model in the hosting environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 20\n",
    "endpoint_config_name = f\"{prefix}-endpoint-config\"+ strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "\n",
    "create_ep_config_response = sagemaker_boto_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"InstanceType\": endpoint_instance_type,\n",
    "            \"InitialVariantWeight\": 1,\n",
    "            \"InitialInstanceCount\": endpoint_instance_count,\n",
    "            \"ModelName\": model_name,\n",
    "            \"VariantName\": \"AllTraffic\",\n",
    "        }\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Create/Update endpoint \n",
    "We create or update existing endpoint with the endpoint configuration that was created in the earlier stage.\n",
    "The endpoint creation will approximately take 5 mins complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 21\n",
    "existing_endpoints = sagemaker_boto_client.list_endpoints(\n",
    "    NameContains=endpoint_name, MaxResults=30)[\"Endpoints\"]\n",
    "if not existing_endpoints:\n",
    "    create_endpoint_response = sagemaker_boto_client.create_endpoint(\n",
    "        EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name\n",
    "    )\n",
    "\n",
    "endpoint_info = sagemaker_boto_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "endpoint_status = endpoint_info[\"EndpointStatus\"]\n",
    "\n",
    "while endpoint_status == \"Creating\":\n",
    "    endpoint_info = sagemaker_boto_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    endpoint_status = endpoint_info[\"EndpointStatus\"]\n",
    "    print(\"Endpoint status:\", endpoint_status)\n",
    "    if endpoint_status == \"Creating\":\n",
    "        time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictor interface\n",
    "Make real-time predictions against SageMaker endpoints with Python objects. We will use data from `dataset.csv` to test predictios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 22\n",
    "predictor = sagemaker.predictor.Predictor(\n",
    "    endpoint_name=endpoint_name, sagemaker_session=sagemaker_session\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policydataset = pd.read_csv(\"./data/dataset.csv\")\n",
    "sample_policy_id = int(policydataset.sample(1)[\"policy_id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calling Predictor API\n",
    " Making inference request using Claims Customer data that is present in the CSV format. In production environment, application that calls the endpint will send the  will receive the customer data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 23\n",
    "# Read claims-customer.csv file in dataframe and list columns\n",
    "dataset = pd.read_csv(\"./data/claims_customer.csv\")\n",
    "col_order = [\"fraud\"] + list(dataset.drop([\"fraud\", \"Unnamed: 0\", \"policy_id\"], axis=1).columns)\n",
    "col_order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pull customer data and format the datapoint.\n",
    "\n",
    "When a customer submits an insurance claim online for instant approval, the insurance company will need to pull customer-specific data. You can do it either using the customer data we have stored in a CSV files or an online feature store to add to the claim data. The pulled data will serve as input for a model prediction.\n",
    "\n",
    "Then, the datapoint must match the exact input format as the model was trained–with all features in the correct order. In this example, the col_order variable was saved when you created the train and test datasets earlier in the guide.\n",
    "\n",
    "We will get sample policy id from the dataset and fetch relevant records from the claims-customer dataset that was created as part of Lab1. You can also fetch the record from feature store and then call the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 24\n",
    "sample_policy_id = int(policydataset.sample(1)[\"policy_id\"])\n",
    "customer_claim_df = dataset[dataset[\"policy_id\"] == sample_policy_id].sample(1)\n",
    "blended_df = customer_claim_df.loc[:, col_order].drop(\"fraud\", axis=1).T.reset_index()\n",
    "blended_df.columns = [\"FeatureName\", \"ValueAsString\"]\n",
    "\n",
    "data_input = \",\".join([str(x) for x in blended_df[\"ValueAsString\"]])\n",
    "data_input\n",
    "print(policydataset.sample(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run prediction on sample claim dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 25\n",
    "results = predictor.predict(data_input, initial_args={\"ContentType\": \"text/csv\"})\n",
    "prediction = json.loads(results)\n",
    "print(f\"The probability claim from policy {int(sample_policy_id)} is fraudulent:\", prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Clean-up\n",
    "\n",
    "---\n",
    "\n",
    "If you are done with this notebook, please run the cell below.  This will remove the hosted endpoint you created and avoid any charges from a stray instance being left on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 26\n",
    "predictor.delete_endpoint(delete_endpoint_config=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Congratulations!** \n",
    "\n",
    "You have successfully completed Lab 3 & 4."
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "ea104a27ec0bfe4dd75a8041bbdf2f96213994c7eab885a59dc565823523111b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
