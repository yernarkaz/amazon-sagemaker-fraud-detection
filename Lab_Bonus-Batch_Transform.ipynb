{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Lab: Batch Transform for on-demand inference\n",
    "\n",
    "This section provides an introduction to the Amazon SageMaker Batch Transform functionality, which are good for the following scenarios:\n",
    "- On-demand model evaluations: if you want to run one time model evaluations. For example, to compare accuracy of our trained model on new validation data that we collected after our initial training job.\n",
    "- Transitional model output: we may want to use outputs from one model as the inputs to another. For example, we may want use a pre-processing step like word embeddings, principal components, clustering, or TF-IDF, before training a second model to generate predictions from that information.\n",
    "- Periodic inference: in some cases, the inference is scheduled to run periodically at a certain time (e.g., nightly batch inference). In this case, running inference via Batch Transform will save cost as compared to hosting an SageMaker inference endpoint, which runs 24/7.\n",
    "\n",
    "Functionally, batch transform uses the same mechanics as real-time hosting to generate predictions. It requires a web server that takes in HTTP POST requests a single observation, or mini-batch, at a time. However, unlike real-time hosted endpoints which have persistent hardware (instances stay running until you shut them down), batch transform clusters are torn down when the job completes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a continuation of Lab 4 after the model was created. You will need to update the model name in this notebook. If you don't have a model created, please review Lab 4 (Model deployment), Section \"Create model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 1\n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from time import gmtime, strftime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 2\n",
    "\n",
    "# Set Region\n",
    "region = sagemaker.Session().boto_region_name\n",
    "print(\"Using AWS Region: {}\".format(region))\n",
    "\n",
    "boto3.setup_default_session(region_name=region)\n",
    "boto_session = boto3.Session(region_name=region)\n",
    "s3_client = boto3.client(\"s3\", region_name=region)\n",
    "\n",
    "# Get SageMaker client, role and session\n",
    "sagemaker_boto_client = boto_session.client(\"sagemaker\")\n",
    "sagemaker_role = sagemaker.get_execution_role()\n",
    "sagemaker_session = sagemaker.session.Session(\n",
    "    boto_session=boto_session, sagemaker_client=sagemaker_boto_client\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set parameterized variables:\n",
    "* bucket - S3 Bucket name. You can adjust the code to use a bucket of your choice.\n",
    "* prefix - String which will be used to identify different resources.\n",
    "* model name - String which will be used to identify the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 3\n",
    "\n",
    "# Bucket \n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = \"fraud-detect-demo\"\n",
    "model_name = ADD_YOUR_MODEL_NAME_HERE\n",
    "print(\"Model name : {}\".format(model_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare data input for Batch Transform job\n",
    "We will prepare batch data input by removing the predicted value column and other columns, the index, and header row from raw test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 4\n",
    "\n",
    "dataset = pd.read_csv(\"./data/claims_customer.csv\")\n",
    "batchinput = dataset.drop([\"fraud\", \"Unnamed: 0\", \"policy_id\"], axis=1)\n",
    "batchinput.head()\n",
    "\n",
    "# Save back to CSV without index and header (pre-requisites for Batch Transform job data input)\n",
    "batchinput.to_csv(\"data/batchinput.csv\", index=False, header=False)\n",
    "\n",
    "# Upload to S3 bucket\n",
    "s3_client.upload_file(Filename=\"data/batchinput.csv\", Bucket=bucket, Key=f\"{prefix}/batch_transform/input/batchinput.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create and run Batch Transform job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# cell 5\n",
    "\n",
    "# Batch Transform to evaluate trained model with test data\n",
    "batch_job_name = f\"{prefix}-\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())  # use input data without ID column\n",
    "batch_input = f\"s3://{bucket}/{prefix}/batch_transform/input/batchinput.csv\"\n",
    "batch_output = \"s3://{}/{}/batch_transform/output/{}\".format(bucket, prefix, batch_job_name)\n",
    "\n",
    "request = {\n",
    "    \"TransformJobName\": batch_job_name,\n",
    "    \"ModelName\": model_name,\n",
    "    \"MaxConcurrentTransforms\": 6,\n",
    "    \"BatchStrategy\": \"MultiRecord\",\n",
    "    \"TransformOutput\": {\n",
    "        \"S3OutputPath\": batch_output,\n",
    "        \"Accept\": \"text/csv\",\n",
    "        \"AssembleWith\": \"Line\",\n",
    "    },\n",
    "    \"TransformInput\": {\n",
    "        \"DataSource\": {\"S3DataSource\": {\"S3DataType\": \"S3Prefix\", \"S3Uri\": batch_input}},\n",
    "        \"ContentType\": \"text/csv\",\n",
    "        \"SplitType\": \"Line\",\n",
    "        \"CompressionType\": \"None\",\n",
    "    },\n",
    "    \"TransformResources\": {\"InstanceType\": \"ml.m4.xlarge\", \"InstanceCount\": 1},\n",
    "}\n",
    "\n",
    "response = sagemaker_boto_client.create_transform_job(**request)\n",
    "print(\"Created Transform job with name: \", batch_job_name)\n",
    "\n",
    "# Wait until the job finishes\n",
    "try:\n",
    "    sagemaker_boto_client.get_waiter(\"transform_job_completed_or_stopped\").wait(TransformJobName=batch_job_name)\n",
    "finally:\n",
    "    response = sagemaker_boto_client.describe_transform_job(TransformJobName=batch_job_name)\n",
    "    status = response[\"TransformJobStatus\"]\n",
    "    print(\"Transform job ended with status: \" + status)\n",
    "    if status == \"Failed\":\n",
    "        message = response[\"FailureReason\"]\n",
    "        print(\"Transform failed with the following error: {}\".format(message))\n",
    "        raise Exception(\"Transform job failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading the prediction results from Batch Transform job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Batch Transform job outputs the batch prediction results in the S3 location defined in 'batch_output'. We will merge the prediction column with the original sample input dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 6\n",
    "\n",
    "def get_csv_output_from_s3(s3uri, batch_file):\n",
    "    file_name = \"{}.out\".format(batch_file)\n",
    "    match = re.match(\"s3://([^/]+)/(.*)\", \"{}/{}\".format(s3uri, file_name))\n",
    "    output_bucket, output_prefix = match.group(1), match.group(2)    \n",
    "    s3_client.download_file(output_bucket, output_prefix, file_name)\n",
    "    return pd.read_csv(file_name, sep=\",\", header=None)\n",
    "\n",
    "y_predict = get_csv_output_from_s3(batch_output, \"batchinput.csv\")\n",
    "y_predict.columns = ['prediction']\n",
    "predict = pd.concat([y_predict.round().astype(int), dataset], axis=1)\n",
    "predict.drop([\"Unnamed: 0\"], axis=1, inplace=True)\n",
    "predict.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You've successfully completed the bonus lab - Batch Transform for on-demand inference"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "ea104a27ec0bfe4dd75a8041bbdf2f96213994c7eab885a59dc565823523111b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
