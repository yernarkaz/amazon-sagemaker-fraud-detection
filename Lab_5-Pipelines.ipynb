{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5: Create an end to end Machine Learning Workflow using SageMaker Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this notebook, you will build a machine learning workflow using SageMaker Pipeline that automates end-to-end process of data preparation, model training, and deploying that detects fraudulent automobile insurance claims. SageMaker Pipelines is a series of interconnected steps that are defined using the [Pipelines SDK](https://sagemaker.readthedocs.io/en/stable/workflows/pipelines/sagemaker.workflow.pipelines.html). These steps define the actions that the pipeline takes and the relationships between these steps is defined using properties. We will use the pipeline steps to configure and create end to end machine learning workflow.\n",
    "\n",
    "## Index\n",
    "\n",
    "---\n",
    "\n",
    "1. [Prerequisites](#Prerequisites)\n",
    "1. [Architecture](#Architecture)\n",
    "1. [Creating a machine learing workflow using SageMaker Pipeline](#Creating-a-machine-learing-workflow-using-SageMaker-Pipeline)\n",
    "1. [Clean-Up](#Clean-Up)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install required third-party libraries. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default SageMaker notebook container already has many of the packages we'll need e.g Pandas, Numpy and Matplotlib. \n",
    "\n",
    "The following extra python modules will be installed in the next cell:\n",
    "1. `sagemaker`: Python SDK to call the SageMaker API\n",
    "2. `boto3`: Python SDK to call the AWS API\n",
    "\n",
    "(if you encounter a warning about pip running as root user - don't worry about it. We're running everything in a container, so it's not going to break anything) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1\n",
    "!python -m pip install -Uq pip\n",
    "!python -m pip install -q sagemaker==2.117.0 boto3==1.24.62"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2\n",
    "import json\n",
    "import boto3\n",
    "import pathlib\n",
    "import sagemaker\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import time\n",
    "\n",
    "import scripts\n",
    "\n",
    "from sagemaker.xgboost.estimator import XGBoost\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.steps import CreateModelStep\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.workflow.step_collections import RegisterModel\n",
    "from sagemaker.workflow.steps import ProcessingStep, TrainingStep\n",
    "from sagemaker.workflow.parameters import ParameterInteger, ParameterFloat, ParameterString\n",
    "from sagemaker.workflow.lambda_step import (\n",
    "    LambdaStep,\n",
    "    LambdaOutput,\n",
    "    LambdaOutputTypeEnum,\n",
    ")\n",
    "from sagemaker.lambda_helper import Lambda\n",
    "from sagemaker import image_uris\n",
    "from scripts.demo_helpers import delete_project_resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to load the variables stored from previous notebooks, we need to run the following storemagic cell command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3\n",
    "%store -r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set region and boto3 config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4\n",
    "# You can change this to a region of your choice\n",
    "region = sagemaker.Session().boto_region_name\n",
    "print(\"Using AWS Region: {}\".format(region))\n",
    "\n",
    "boto3.setup_default_session(region_name=region)\n",
    "boto_session = boto3.Session(region_name=region)\n",
    "\n",
    "s3_client = boto3.client(\"s3\", region_name=region)\n",
    "\n",
    "sagemaker_boto_client = boto_session.client(\"sagemaker\")\n",
    "sagemaker_session = sagemaker.session.Session(\n",
    "    boto_session=boto_session, sagemaker_client=sagemaker_boto_client\n",
    ")\n",
    "sagemaker_role = sagemaker.get_execution_role()\n",
    "\n",
    "account_id = boto3.client(\"sts\").get_caller_identity()[\"Account\"]\n",
    "\n",
    "# This variables has been stored in the previous notebooks\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "print(\"Bucket: {}\".format(bucket))\n",
    "prefix = \"fraud-detect-demo\"\n",
    "print(\"Prefix: {}\".format(prefix))\n",
    "\n",
    "claims_fg_name = f\"{prefix}-claims\"\n",
    "customers_fg_name = f\"{prefix}-customers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5\n",
    "# ======> Tons of output_paths\n",
    "\n",
    "training_job_output_path = f\"s3://{bucket}/{prefix}/training_jobs\"\n",
    "bias_report_output_path = f\"s3://{bucket}/{prefix}/clarify-bias\"\n",
    "explainability_output_path = f\"s3://{bucket}/{prefix}/clarify-explainability\"\n",
    "\n",
    "train_data_uri = f\"s3://{bucket}/{prefix}/data/train/train.csv\"\n",
    "test_data_uri = f\"s3://{bucket}/{prefix}/data/test/test.csv\"\n",
    "processing_dir = \"/opt/ml/processing\"\n",
    "create_dataset_script_uri = \"scripts/create_dataset.py\"\n",
    "pipeline_bias_output_path = f\"s3://{bucket}/{prefix}/clarify-output/pipeline/bias\"\n",
    "\n",
    "# ======> variables used for parameterizing the notebook run\n",
    "flow_instance_count = 1\n",
    "flow_instance_type = \"ml.m5.4xlarge\"\n",
    "\n",
    "train_instance_count = 1\n",
    "train_instance_type = \"ml.m4.xlarge\"\n",
    "\n",
    "deploy_model_instance_type = \"ml.m5.xlarge\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "\n",
    "---\n",
    "\n",
    "![End to end pipeline architecture](images/notebooks/e2e-pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Creating a machine learing workflow using SageMaker Pipeline\n",
    "\n",
    "---\n",
    "\n",
    "- [Step 0: Data preparation prerequisite](#Step-0:-Data-preparation-prerequisite)\n",
    "- [Step 1: Claims Data Wrangler preprocessing step](#Step-1:-Claims-Data-Wrangler-preprocessing-step)\n",
    "- [Step 2: Customers Data Wrangler preprocessing step](#Step-2:-Customers-Data-Wrangler-preprocessing-step)\n",
    "- [Step 3: Create dataset and train/test split](#Step-3:-Create-dataset-and-train/test-split)\n",
    "- [Step 4: Train XGBoost model](#Step-4:-Train-XGBoost-model)\n",
    "- [Step 5: Model pre-Deployment step](#Step-5:-Model-pre-deployment-step)\n",
    "- [Step 6: Register the model](#Step-6:-Register-the-model)\n",
    "- [Step 7: Model deployment](#Step-7:-Model-deployment)\n",
    "- [Step 8: Combine and run the Pipeline steps](#Step-8:-Combine-and-run-the-Pipeline-steps)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section you will create a pipeline which trains a new model, persists the model in SageMaker and then adds the model to the registry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 0: Data preparation prerequisite\n",
    "If you have successfully completed labs 1 through 4, then you can skip this step and proceed with [Step 1](#Step-1:-Claims-Data-Wrangler-Preprocessing-Step). \n",
    "\n",
    "#### Create Feature Store\n",
    "If you choose to run this step, it will take approximately 5 mins to complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy and paste the bucket name and the prefix on the next cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6\n",
    "!python3 scripts/create_feature_store.py --bucket $bucket --region $region --prefix \"fraud-detect-demo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload raw data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7\n",
    "s3_client.upload_file(\n",
    "    Filename=\"data/claims.csv\", Bucket=bucket, Key=f\"{prefix}/data/raw/claims.csv\"\n",
    ")\n",
    "s3_client.upload_file(\n",
    "    Filename=\"data/customers.csv\", Bucket=bucket, Key=f\"{prefix}/data/raw/customers.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Claims Data Wrangler preprocessing step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Pipeline parameters\n",
    "An important feature of SageMaker Pipelines is the ability to define the steps ahead of time, but also to be able to change the parameters to those steps at execution without having to re-define the pipeline. \n",
    "\n",
    "This can be achieved by using `ParameterInteger`, `ParameterFloat` or `ParameterString` to define a value initally which can be modified when you call `pipeline.start(parameters=parameters)` later. Only certain parameters can be defined this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8\n",
    "train_instance_param = ParameterString(\n",
    "    name=\"TrainingInstance\",\n",
    "    default_value=\"ml.m4.xlarge\",\n",
    ")\n",
    "\n",
    "model_approval_status = ParameterString(\n",
    "    name=\"ModelApprovalStatus\", default_value=\"PendingManualApproval\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update attributes within the .flow file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A quick introduction to `.flow` files\n",
    "\n",
    "We use Amazon SageMaker Data Wrangler to create transforms on datasets. The transforms are exported as a `.flow` file which can be shared between teams. These .flow files are specialized json files that be read in using python's `json` library. Using these files one can apply transforms to a dataset by running a Data Wrangler job. If you wish to learn more about this process, please navigate to the bonus material: Data exploration using Amazon SageMaker Data Wrangler in workshop studio.\n",
    "\n",
    "We have already provided you with flow templates for the claims and customer dataset. These are `claims_flow_template` and `customers_flow_template` under the `outputs` folder respectively. These files contain references to the S3 bucket used when these files were created.\n",
    "\n",
    "<font color=\"orange\">You will now need to change the pre-defined S3 bucket references to your own S3 bucket to use these files</font>\n",
    "\n",
    "Once the cell below is executed, you can open the `claims.flow` and `customers.flow` files and export the data to S3 or you can continue the guide using the provided `data/claims_preprocessed.csv` and `data/customers_preprocessed.csv` files. The latter is the recommended approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9\n",
    "claims_flow_template_file = \"outputs/claims_flow_template\"\n",
    "\n",
    "# Open claims_flow_template and substitute the new S3 bucket and prefix\n",
    "with open(claims_flow_template_file, \"r\") as f:\n",
    "    variables = {\"bucket\": bucket, \"prefix\": prefix}\n",
    "    template = string.Template(f.read())\n",
    "    claims_flow = template.substitute(variables)\n",
    "    claims_flow = json.loads(claims_flow)\n",
    "\n",
    "# then write it to outputs/claims.flow \n",
    "with open(\"outputs/claims.flow\", \"w\") as f:\n",
    "    json.dump(claims_flow, f)\n",
    "\n",
    "\n",
    "customers_flow_template_file = \"outputs/customers_flow_template\"\n",
    "\n",
    "# Next do the same thing with customers_flow_template and substitute the new S3 bucket and prefix\n",
    "with open(customers_flow_template_file, \"r\") as f:\n",
    "    variables = {\"bucket\": bucket, \"prefix\": prefix}\n",
    "    template = string.Template(f.read())\n",
    "    customers_flow = template.substitute(variables)\n",
    "    customers_flow = json.loads(customers_flow)\n",
    "\n",
    "# Write it to the outputs/customers.flow file\n",
    "with open(\"outputs/customers.flow\", \"w\") as f:\n",
    "    json.dump(customers_flow, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use `claims.flow` and `customers.flow` files. Both these files should now be available in the folder `outputs`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the first Data Wrangler step's inputs\n",
    "\n",
    "Upload the claims.flow file to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10\n",
    "s3_client.upload_file(\n",
    "    Filename=\"outputs/claims.flow\", Bucket=bucket, Key=f\"{prefix}/dataprep-notebooks/claims.flow\"\n",
    ")\n",
    "print(f\"Claims flow file uploaded to S3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11\n",
    "# the path to the S3 claims flow file \n",
    "claims_flow_uri = f\"s3://{bucket}/{prefix}/dataprep-notebooks/claims.flow\"\n",
    "\n",
    "# read in the claims.flow file\n",
    "with open(\"outputs/claims.flow\", \"r\") as f:\n",
    "    claims_flow = json.load(f)\n",
    "\n",
    "flow_step_inputs = []\n",
    "\n",
    "# flow file contains the code for each transformation\n",
    "flow_file_input = sagemaker.processing.ProcessingInput(\n",
    "    source=claims_flow_uri, destination=f\"{processing_dir}/flow\", input_name=\"flow\"\n",
    ")\n",
    "\n",
    "flow_step_inputs.append(flow_file_input)\n",
    "\n",
    "# parse the flow file for S3 inputs to Data Wranger job\n",
    "for node in claims_flow[\"nodes\"]:\n",
    "    if \"dataset_definition\" in node[\"parameters\"]:\n",
    "        data_def = node[\"parameters\"][\"dataset_definition\"]\n",
    "        name = data_def[\"name\"]\n",
    "        s3_input = sagemaker.processing.ProcessingInput(\n",
    "            source=data_def[\"s3ExecutionContext\"][\"s3Uri\"],\n",
    "            destination=f\"{processing_dir}/{name}\",\n",
    "            input_name=name,\n",
    "        )\n",
    "        flow_step_inputs.append(s3_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define outputs for first Data Wranger step\n",
    "\n",
    "The flow file can be loaded as a python dictionary. The output name is a string with the variables from the claims.flow file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12\n",
    "claims_output_name = (\n",
    "    f\"{claims_flow['nodes'][-1]['node_id']}.{claims_flow['nodes'][-1]['outputs'][0]['name']}\"\n",
    ")\n",
    "\n",
    "print(f\"Claims output name is {claims_output_name}\")\n",
    "\n",
    "flow_step_outputs = []\n",
    "\n",
    "# create the ProcessingOutput\n",
    "flow_output = sagemaker.processing.ProcessingOutput(\n",
    "    output_name=claims_output_name,\n",
    "    feature_store_output=sagemaker.processing.FeatureStoreOutput(feature_group_name=claims_fg_name),\n",
    "    app_managed=True,\n",
    ")\n",
    "\n",
    "flow_step_outputs.append(flow_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define processor and processing step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a processing workflow step to create a job for data processing. For more information on processing jobs, see [Process Data and Evaluate Model](https://docs.aws.amazon.com/sagemaker/latest/dg/processing-job.html).\n",
    "\n",
    "A processing step requires a processor, a Python script that defines the processing code, outputs for processing, and job arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13\n",
    "# Pulls the latest data-wrangler container tag, i.e. \"1.x\"\n",
    "image_uri = image_uris.retrieve(framework=\"data-wrangler\", region=region)\n",
    "\n",
    "print(\"image_uri: {}\".format(image_uri))\n",
    "\n",
    "flow_processor = sagemaker.processing.Processor(\n",
    "    role=sagemaker_role,\n",
    "    image_uri=image_uri,\n",
    "    instance_count=flow_instance_count,\n",
    "    instance_type=flow_instance_type,\n",
    "    max_runtime_in_seconds=86400,\n",
    ")\n",
    "\n",
    "output_content_type = \"CSV\"\n",
    "\n",
    "# Output configuration used as processing job container arguments\n",
    "claims_output_config = {claims_output_name: {\"content_type\": output_content_type}}\n",
    "\n",
    "claims_flow_step = ProcessingStep(\n",
    "    name=\"ClaimsDataWranglerProcessingStep\",\n",
    "    processor=flow_processor,\n",
    "    inputs=flow_step_inputs,\n",
    "    outputs=flow_step_outputs,\n",
    "    job_arguments=[f\"--output-config '{json.dumps(claims_output_config)}'\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Customers Data Wrangler preprocessing step\n",
    "\n",
    "Similarly upload the customers.flow file to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14\n",
    "s3_client.upload_file(\n",
    "    Filename=\"outputs/customers.flow\", Bucket=bucket, Key=f\"{prefix}/dataprep-notebooks/customers.flow\"\n",
    ")\n",
    "\n",
    "print(f\"Customers flow file uploaded to S3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 15\n",
    "# the s3 data uri\n",
    "customers_flow_uri = f\"s3://{bucket}/{prefix}/dataprep-notebooks/customers.flow\"\n",
    "\n",
    "# read in the flow file\n",
    "with open(\"outputs/customers.flow\", \"r\") as f:\n",
    "    customers_flow = json.load(f)\n",
    "\n",
    "flow_step_inputs = []\n",
    "\n",
    "# create a ProcessingInput step\n",
    "# flow file contains the code for each transformation\n",
    "flow_file_input = sagemaker.processing.ProcessingInput(\n",
    "    source=customers_flow_uri, destination=f\"{processing_dir}/flow\", input_name=\"flow\"\n",
    ")\n",
    "\n",
    "flow_step_inputs.append(flow_file_input)\n",
    "\n",
    "# parse the flow file for S3 inputs to Data Wranger job\n",
    "for node in customers_flow[\"nodes\"]:\n",
    "    if \"dataset_definition\" in node[\"parameters\"]:\n",
    "        data_def = node[\"parameters\"][\"dataset_definition\"]\n",
    "        name = data_def[\"name\"]\n",
    "        s3_input = sagemaker.processing.ProcessingInput(\n",
    "            source=data_def[\"s3ExecutionContext\"][\"s3Uri\"],\n",
    "            destination=f\"{processing_dir}/{name}\",\n",
    "            input_name=name,\n",
    "        )\n",
    "        flow_step_inputs.append(s3_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 16\n",
    "# output name is created from variables found within the flow file which can be loaded as a dict.\n",
    "customers_output_name = (\n",
    "    f\"{customers_flow['nodes'][-1]['node_id']}.{customers_flow['nodes'][-1]['outputs'][0]['name']}\"\n",
    ")\n",
    "\n",
    "print(f\"Customers output name is {claims_output_name}\")\n",
    "\n",
    "flow_step_outputs = []\n",
    "\n",
    "flow_output = sagemaker.processing.ProcessingOutput(\n",
    "    output_name=customers_output_name,\n",
    "    feature_store_output=sagemaker.processing.FeatureStoreOutput(\n",
    "        feature_group_name=customers_fg_name\n",
    "    ),\n",
    "    app_managed=True,\n",
    ")\n",
    "\n",
    "flow_step_outputs.append(flow_output)\n",
    "\n",
    "output_content_type = \"CSV\"\n",
    "\n",
    "# Output configuration used as processing job container arguments\n",
    "customers_output_config = {customers_output_name: {\"content_type\": output_content_type}}\n",
    "\n",
    "# define a processing step\n",
    "customers_flow_step = ProcessingStep(\n",
    "    name=\"CustomersDataWranglerProcessingStep\",\n",
    "    processor=flow_processor,\n",
    "    inputs=flow_step_inputs,\n",
    "    outputs=flow_step_outputs,\n",
    "    job_arguments=[f\"--output-config '{json.dumps(customers_output_config)}'\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Create dataset and train/test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will use SageMaker Pipelines `ProcessingStep` step to split the dataset into training and testing, using `SKLearnProcessor` processor. You can split the dataset without using processing step, but in case of large datasets, it is recommended to use the managed processing jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 17\n",
    "create_dataset_processor = SKLearnProcessor(\n",
    "    framework_version=\"0.23-1\",\n",
    "    role=sagemaker_role,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    instance_count=1,\n",
    "    base_job_name=\"fraud-detection-demo-create-dataset\",\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")\n",
    "\n",
    "create_dataset_step = ProcessingStep(\n",
    "    name=\"CreateDataset\",\n",
    "    processor=create_dataset_processor,\n",
    "    outputs=[\n",
    "        sagemaker.processing.ProcessingOutput(\n",
    "            output_name=\"train_data\", source=\"/opt/ml/processing/output/train\"\n",
    "        ),\n",
    "        sagemaker.processing.ProcessingOutput(\n",
    "            output_name=\"test_data\", source=\"/opt/ml/processing/output/test\"\n",
    "        ),\n",
    "    ],\n",
    "    job_arguments=[\n",
    "        \"--claims-feature-group-name\",\n",
    "        claims_fg_name,\n",
    "        \"--customers-feature-group-name\",\n",
    "        customers_fg_name,\n",
    "        \"--bucket-name\",\n",
    "        bucket,\n",
    "        \"--bucket-prefix\",\n",
    "        prefix,\n",
    "        \"--region\",\n",
    "        region,\n",
    "    ],\n",
    "    code=create_dataset_script_uri,\n",
    "    depends_on=[claims_flow_step.name, customers_flow_step.name],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Train XGBoost model\n",
    "\n",
    "In this step, we will use Amazon SageMaker's XGBoost Algorithm to train on this dataset. Configure an Estimator for the XGBoost algorithm and the input dataset. A typical training script loads data from the input channels, configures training with hyperparameters, trains a model, and saves a model to model_dir so that it can be hosted later. You also specify the output path where trained model is saved. We will use the ParameterString `train_instance_param` defined at the beginning of the pipeline. Note the instance_type parameter may be used in multiple places in the pipeline. In this case, the instance_type is passed into the estimator.\n",
    "\n",
    "Finally, use the estimator instance to construct a `TrainingStep` as well as the `properties` of the prior `ProcessingStep` used as input in the `TrainingStep` inputs and the code that's executed when the pipeline invokes the pipeline execution. This is similar to an estimator's fit method in the Python SDK.\n",
    "\n",
    "Pass in the `S3Uri` of the `\"train_data\"` output channel to the `TrainingStep`. Also, use the other \"test_data\" output channel for model evaluation in the pipeline. The properties attribute of a Pipeline step matches the object model of the corresponding response of a describe call. These properties can be referenced as placeholder values and are resolved at runtime. For example, the `ProcessingStep` `properties` attribute matches the object model of the [DescribeTrainingJob](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_DescribeTrainingJob.html)  response object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 18\n",
    "hyperparameters = {\n",
    "    \"max_depth\": \"3\",\n",
    "    \"eta\": \"0.2\",\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"num_round\": \"100\",\n",
    "}\n",
    "\n",
    "xgb_estimator = XGBoost(\n",
    "    entry_point=\"scripts/xgboost_starter_script.py\",\n",
    "    output_path=training_job_output_path,\n",
    "    code_location=training_job_output_path,\n",
    "    hyperparameters=hyperparameters,\n",
    "    role=sagemaker_role,\n",
    "    instance_count=train_instance_count,\n",
    "    instance_type=train_instance_param.default_value,\n",
    "    framework_version=\"1.0-1\",\n",
    ")\n",
    "\n",
    "train_step = TrainingStep(\n",
    "    name=\"XgboostTrain\",\n",
    "    estimator=xgb_estimator,\n",
    "    inputs={\n",
    "        \"train\": sagemaker.inputs.TrainingInput(\n",
    "            s3_data=create_dataset_step.properties.ProcessingOutputConfig.Outputs[\n",
    "                \"train_data\"\n",
    "            ].S3Output.S3Uri\n",
    "        )\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Model pre-deployment step\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifically, pass in the `S3ModelArtifacts` from the `TrainingStep`, `step_train` properties. The `TrainingStep` `properties` attribute matches the object model of the [DescribeTrainingJob](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_DescribeTrainingJob.html) response object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 19\n",
    "model = sagemaker.model.Model(\n",
    "    name=\"fraud-detection-demo-pipeline-xgboost\",\n",
    "    image_uri=train_step.properties.AlgorithmSpecification.TrainingImage,\n",
    "    model_data=train_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    role=sagemaker_role,\n",
    ")\n",
    "\n",
    "inputs = sagemaker.inputs.CreateModelInput(instance_type=\"ml.m4.xlarge\")\n",
    "\n",
    "create_model_step = CreateModelStep(name=\"ModelPreDeployment\", model=model, inputs=inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Register the model\n",
    "In this step you will use the ParameterString `model_approval_status` defined at the outset of the pipeline code.\n",
    "\n",
    "Use the estimator instance specified in the training step to construct an instance of `RegisterModel`. The result of executing `RegisterModel` in a pipeline is a model package. A model package is an abstraction of reusable model artifacts that packages all ingredients required for inference. Primarily, it consists of an inference specification that defines the inference image to use along with an optional model weights location.\n",
    "\n",
    "A model package group is a collection of model packages. A model package group can be created for a specific ML business problem, and new versions of the model packages can be added to it. Typically, customers are expected to create a ModelPackageGroup for a SageMaker pipeline so that model package versions can be added to the group for every SageMaker Pipeline run.\n",
    "\n",
    "The construction of `RegisterModel` is similar to an estimator instance's `register` method in the Python SDK.\n",
    "\n",
    "Specifically, pass in the `S3ModelArtifacts` from the `TrainingStep`, step_train properties. The TrainingStep properties attribute matches the object model of the DescribeTrainingJob response object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 20\n",
    "mpg_name = prefix\n",
    "\n",
    "\n",
    "register_step = RegisterModel(\n",
    "    name=\"XgboostRegisterModel\",\n",
    "    estimator=xgb_estimator,\n",
    "    model_data=train_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    content_types=[\"text/csv\"],\n",
    "    response_types=[\"text/csv\"],\n",
    "    inference_instances=[\"ml.t2.medium\", \"ml.m5.xlarge\"],\n",
    "    transform_instances=[\"ml.m5.xlarge\"],\n",
    "    model_package_group_name=mpg_name,\n",
    "    approval_status=model_approval_status,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Model deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is registered, the next step is deploying the model. In a typical MLOps scenario, model building pipeline is separate from model deployment pipeline. However, using Lambda function step, You can choose to deploy the model as part of the SageMaker Pipelines step.\n",
    "\n",
    "The SageMaker SDK provides a Lambda helper class that can be used to create a Lambda function. This function is provided to the Lambda step for invocation via the pipeline. Alternatively, a predefined Lambda function can be provided to the Lambda step.\n",
    "\n",
    "In this step, you will work with a pre-created Lambda function that was already provisioned by the cloudformation script.\n",
    "The SageMaker Execution Role requires the policy `AmazonSageMakerPipelinesIntegrations` to create the Lambda function, and the Lambda function needs a role with policies allowing creation of a SageMaker endpoint.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copy Lambda ARN from Cloud Formation Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\">Note: Please open [CloudFormation console](https://console.aws.amazon.com/cloudformation/home) and copy Lambda ARN from the generated function (under the Output tab). </font>\n",
    "    \n",
    "<font color=\"orange\">This is a mandatory step to proceed further.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 21\n",
    "function_arn=\"ADD YOUR ARN HERE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define the Lambda step in the next block. After defining object names, we use the pre-created Lambda function by passing its ARN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 22\n",
    "# Use the current time to define unique names for the resources created\n",
    "current_time = time.strftime(\"%m-%d-%H-%M-%S\", time.localtime())\n",
    "\n",
    "model_name = create_model_step.properties.ModelName\n",
    "endpoint_config_name = \"fraud-endpoint-config\" + current_time\n",
    "endpoint_name = \"fraud-endpoint-\" + current_time\n",
    "function_name = \"sagemaker-fraud-lambda-step\" + current_time\n",
    "\n",
    "# The dictionary retured by the Lambda function is captured by LambdaOutput, each key in the dictionary corresponds to a\n",
    "# LambdaOutput\n",
    "output_param_1 = LambdaOutput(output_name=\"statusCode\", output_type=LambdaOutputTypeEnum.String)\n",
    "output_param_2 = LambdaOutput(output_name=\"body\", output_type=LambdaOutputTypeEnum.String)\n",
    "output_param_3 = LambdaOutput(output_name=\"other_key\", output_type=LambdaOutputTypeEnum.String)\n",
    "\n",
    "# The inputs provided to the Lambda function can be retrieved via the `event` object within the `lambda_handler` function\n",
    "deploy_step = LambdaStep(\n",
    "    name=\"ModelDeploy\",\n",
    "    lambda_func=Lambda(\n",
    "        function_arn=function_arn\n",
    "    ),\n",
    "    inputs={\n",
    "        \"model_name\": model_name,\n",
    "        \"endpoint_config_name\": endpoint_config_name,\n",
    "        \"endpoint_name\": endpoint_name,\n",
    "    },\n",
    "    outputs=[output_param_1, output_param_2, output_param_3] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Combine and run the Pipeline steps\n",
    "\n",
    "Though easier to reason with, the parameters and steps don't need to be in order. The pipeline DAG will parse it out properly.\n",
    "\n",
    "In this section, we combine the steps into a pipeline so it can be executed.\n",
    "\n",
    "A pipeline requires a `name`, `parameters`, and `steps`. Names must be unique within an `(account, region)` pair.\n",
    "\n",
    "**Note**\n",
    "\n",
    "* All the parameters used in the definitions must be present.\n",
    "* Steps passed into the pipeline do not have to be listed in the order of execution. The SageMaker Pipeline service resolves the data dependency DAG as steps for the execution to complete.\n",
    "* Steps must be unique to across the pipeline step list and all condition step if/else lists.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 23\n",
    "pipeline_name = f\"FraudDetectDemo\"\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[train_instance_param, model_approval_status],\n",
    "    steps=[\n",
    "        claims_flow_step,\n",
    "        customers_flow_step,\n",
    "        create_dataset_step,\n",
    "        train_step,\n",
    "        create_model_step,\n",
    "        register_step,\n",
    "        deploy_step,\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit the pipeline definition to the SageMaker Pipelines service\n",
    "Note: If an existing pipeline has the same name it will be overwritten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 24\n",
    "pipeline.upsert(role_arn=sagemaker_role)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View the entire pipeline definition\n",
    "Viewing the pipeline definition with all the string variables interpolated may help debug pipeline bugs. It is an optional step for this workshop and commented out here. If you want to see how the pipeline is constructed, you can uncomment the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cell 25\n",
    "# json.loads(pipeline.describe()[\"PipelineDefinition\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the pipeline\n",
    "Note this will take about 15 minutes to complete. You can watch the progress of the pipeline job in your SageMaker Studio Components panel. Please feel free to explore additional resources section in the workshop studio while this step is executing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check the visual status of the pipeline:\n",
    "\n",
    "1. Click the Home folder pointed by the arrow and click on `Pipelines`.\n",
    "2. You will see the available pipelines in the table on the right.\n",
    "3. Click on `FraudDetectDemo`.\n",
    "\n",
    "\n",
    "![pipeline-list](images/notebooks/pipeline-navigate.png)\n",
    "\n",
    "Next, you will see the executions listed on the next page. Double-click on the Status `executing` to be taken to the graph representation.\n",
    "\n",
    "\n",
    "![pipeline-execution](images/notebooks/pipeline-executions.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 26\n",
    "# Special pipeline parameters can be defined or changed here\n",
    "parameters = {\"TrainingInstance\": deploy_model_instance_type}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 27\n",
    "start_response = pipeline.start(parameters=parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 28\n",
    "start_response.wait(delay=60, max_attempts=500)\n",
    "start_response.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After completion it will look something like this\n",
    "\n",
    "![Pipeline-success](images/notebooks/pipeline-success.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Up\n",
    "----\n",
    "After running the lab, you should remove the resources which were created. You can also delete all the objects in the project's S3 directory by passing the keyword argument `delete_s3_objects=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 29\n",
    "delete_project_resources(\n",
    "    sagemaker_boto_client=sagemaker_boto_client,\n",
    "    pipeline_name=pipeline_name,\n",
    "    mpg_name=mpg_name,\n",
    "    prefix=prefix,\n",
    "    delete_s3_objects=False,\n",
    "    bucket_name=bucket,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Congratulations!** \n",
    "\n",
    "You have successfully completed lab 5."
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "ea104a27ec0bfe4dd75a8041bbdf2f96213994c7eab885a59dc565823523111b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
